# RAG Project - Summer Semester 2025

> **ðŸŽ“ University Course Chatbot â€“ RAG-Based QA System** 

---

## Overview

This repository hosts the code for a semester-long project on building and experimenting with Retrieval-Augmented Generation (RAG) systems. A chatbot designed to answer university course-related queries using website information and open-source language models.

---

## Folder Structure
<pre> 

baseline/
â”œâ”€â”€ data
â”‚Â Â  â”œâ”€â”€ cleaned_json
â”‚Â Â  â”‚Â Â  â””â”€â”€ *** All cleaned data in json format from university website**
â”‚Â Â  â”‚Â Â  
â”‚Â Â  â””â”€â”€ pdfs
â”‚Â Â      â””â”€â”€ *** Website data in .pdf format is placed here ***
â”œâ”€â”€ evaluation
â”‚Â Â  â””â”€â”€ evaluation_unsupervised.py
â”œâ”€â”€ generator
â”‚Â Â  â””â”€â”€ generator.py
â”œâ”€â”€ llms
â”‚Â Â  â”œâ”€â”€ Place the suitable LLM model here (Dowloadable from https://raw.githubusercontent.com/nomic-ai/gpt4all/main/gpt4all-chat/metadata/models2.json)
â”‚Â Â  â””â”€â”€ README.MD
â”œâ”€â”€ logs
â”‚Â Â  â””â”€â”€ RAGPipeline.log
â”œâ”€â”€ pipeline.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ retriever
â”‚Â Â  â””â”€â”€ retriever_bge_m3.py
â”œâ”€â”€ runPipeline.py
â”œâ”€â”€ test
â”‚Â Â  â”œâ”€â”€ F1_score_calculation.py
â”‚Â Â  â”œâ”€â”€ input
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ f1_qa_input.json
|   |   â”œâ”€â”€ bert_qa_input.json
â”‚Â Â  â”‚Â Â  â””â”€â”€ questions.txt
â”‚Â Â  â”œâ”€â”€ outputs
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ answers.txt
â”‚Â Â  â”‚Â Â  â””â”€â”€ f1_output.txt
â”‚Â Â  â”œâ”€â”€ RAGPipelineTest_bge_m3.py
â”‚Â Â  â””â”€â”€ test_retriever.py
â””â”€â”€ util
    â”œâ”€â”€ fileUtil.py
    â””â”€â”€ logger_config.py



</pre>
## Steps to setup and run from the `baseline` folder
 1. **Make sure your are currently inside the /baseline folder**
 2. **Place the LLM file in the baseline/llms folder:**  https://raw.githubusercontent.com/nomic-ai/gpt4all/main/gpt4all-chat/metadata/models2.json
 3. **To install the required packages:** Run the below commands in the terminal to install the requiered package
                ```bash
                    conda env create -f environment.yml;
                    conda activate rag-bot-test
                 ```
 
 4. **To run the chatbot:** python runPipeline.py
 5. **To access the chatbot UI:** Access the url that is displayed once the server starts. Eg: http://127.0.0.1:xxxx


## Steps to setup and run testcases
 1. **Make sure your are currently inside the /baseline folder**
 2. **To test with new question, add your question to:** test/input/questions.txt
 3. **To run the RAGPipeline with test data/questions:** python test/RAGPipelineTest_bge_m3.py
 5. **Output file:** test/output/answers.txt
 
 
---
## Usage Instructions

## Retriever Module

The `Retriever` class in `retriever.py` implements hybrid document retrieval for RAG systems, combining dense and sparse methods to improve coverage.

1. **Load and index documents** (`.txt`, `.pdf`, `.json`) with overlapping chunking  
2. **Support hybrid retrieval** using dense vectors (BGE-M3) and BM25 with weighted score fusion  
3. **Optional reranking** with a cross-encoder for more accurate results  
4. **Track sources** so each chunk can be traced back to its original file  
5. **Save and load** FAISS and BM25 indexes for reuse


### ðŸ§© Dependencies

The `Retriever` module depends on the following Python packages:

- sentence-transformers
- transformers
- faiss-cpu
- rank-bm25
- PyMuPDF
- loguru

To install:

```bash
pip install sentence-transformers transformers faiss-cpu rank-bm25 PyMuPDF loguru
```


### âœ… Features

- Dense + sparse retrieval with score fusion  
- Optional cross-encoder reranker  
- Overlapping chunking based on tokenizer  
- Supports `.txt`, `.md`, `.pdf`, `.json` files  
- Source tracking for each chunk  
- Easy saving and loading of FAISS/BM25 index


---

## Usage Instructions


## Generator Module

The `Generator` class in `generator.py` utilizes a local LlamaCpp model for generating answers based on retrieved context, which is useful for question-answering systems.

1. **Load llm model**  from a given file path
2. **Generate answers** by combining a user query with relevant context chunks using a customizable prompt template
3. **Handle errors** to manage issues during model loading

## Dependencies

1. **LlamaCpp**: The library used for loading and interacting with the LlamaCpp model.
2. **os**: Standard library for file path operations.
3. **sys**: The model used to manipulate the Python runtime environment, access command-line arguments, and handle standard input/output streams.
4. **List**: In the generate_answer method, List[str] specifies that the context_chunks parameter should be a list of strings.
5. **StreamingStdOutCallbackHandler**: It handles streaming output from the LlamaCpp model, allowing the generated text to be printed to standard output in real-time as it is produced.



## Reflections and Thoughts

We tested our system using both English and German documents. Currently, it only supports querying English documents in English and German documents in German. In the future, we could build on this foundation to enable cross-lingual retrieval.

We can validate input parameters for the Generator class to make sure the output from the model conforms to the expected results. In terms of syntax, the text chunks should not be an empty list or the generated answers should be in the format of string. In terms of semantics, the standard of consine similarity score of query and retrieved chunks can be tested differently to find out the best score to return the most suitable chunks for the preparation of answer generation.

How to handle dynamic data and realize the on-time update in our raw data automatically will be of great significance for the chatbotâ€™s performance. With the help of it, our chatbot can always be supported by the lastest information from the webpages.


---


## Team Members

| Role   | Name            | GitHub Username |
| ------ | --------------- | --------------- |
| Member | Mengmeng Yu     | Linqiaoqiao2    |
| Member | Wenhui Deng     | deng-wenhui     |
| Member | Subhasri Suresh | subhasri-suresh |

---

